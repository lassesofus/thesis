\subsection{Anisotropic planning performance}

The zero-shot reaching experiments show that planning success is not determined solely by how close the current state is to the goal in latent space. Instead, performance depends on how well the learned visual representation and action-conditioned dynamics align with physically meaningful directions of motion. Although the reaching tasks differ only by the axis of movement, planning performance varies significantly across axes. This indicates that the latent-space objective used for planning induces an anisotropic landscape, where the same reduction in latent distance can correspond to very different amounts of physical progress.

Along the $x$-axis, planning performs quite poorly despite large reductions in latent-space distance. Inspection of the optimized actions (second row in the left-most column in Figure \ref{fig:exp01_zero_shot}) shows that the planner often introduces substantial motion along the $y$- and $z$-axes, even though the task requires motion only along $x$. Analysis of the DROID action distribution (Appendix~\ref{app:droid_action_analysis}) confirms that multi-axis coupling is pervasive in the training data, with off-axis motion magnitudes typically amounting to 67--82\% of the primary-axis magnitude. This prevalence of coupled motion suggests that the action-conditioned predictor may have learned an implicit prior favoring multi-axis actions, having rarely observed isolated single-axis displacements during training. However, the specific off-axis movements observed during planning do not match the marginal action statistics from DROID: the dataset exhibits a negative correlation between forward and downward motion, yet the planner often produces positive $z$-components alongside positive $x$-motion. This mismatch indicates that the bias is not a simple replay of training action patterns, but rather emerges from how the learned dynamics model internally associates visual changes with coupled action patterns. When CEM optimizes latent-space distance under this model, it selects actions that appear effective in representation space but are misaligned with the intended task geometry, consistent with the observed non-monotonic Cartesian error trajectories and the weak correspondence between latent-space progress and physical motion for $x$-axis reaching.

For $z$-axis reaching, planning shows moderate average improvement but high variability across episodes. This suggests that the latent-space objective is ambiguous rather than consistently biased. In particular, multiple distinct actions may lead to similar latent-space distances to the goal. In such cases, CEM can converge to different locally optimal solutions depending on random sampling, resulting in similar latent outcomes but different physical trajectories. In contrast, $y$-axis reaching exhibits low variance and nearly monotonic error reduction, indicating a better-conditioned objective in which latent-space gradients align more reliably with the desired physical motion.

Interestingly, along the $y$-axis, the measured latent-space distance consistently increases after the first planning step, even though CEM explicitly optimizes to minimize predicted latent distance. Two factors combine to produce this behavior. First, the action magnitude is bounded to a maximum of 7.5\,cm per step; when the true energy-minimizing action lies outside this range, the planner is constrained to boundary actions. Second, comparisons between predicted and actual latent distances reveal that the dynamics model is systematically optimistic for $y$-axis motion: the predicted energy after executing an action is consistently lower than the actual measured energy. While the action constraint affects all axes equally, this optimistic bias is substantially stronger for $y$-axis motion than for $x$-axis motion. As a result, CEM selects constrained actions while overestimating the progress they will achieve. When these actions are executed, the actual latent distance is higher than anticipated. This combination of action saturation and axis-specific predictor optimism explains both why the latent distance increases after step~1 and why this phenomenon is specific to the $y$-axis. Despite these latent-space inconsistencies, Cartesian distance still decreases monotonically, showing that the planner moves in the correct physical direction even when the latent objective temporarily increases.

To test whether these limitations can be overcome by repeated replanning, the number of CEM planning steps was increased from 5 to 10 (Appendix~\ref{app:long_zero_shot_planning}). This change does not improve performance along the $x$-axis and slightly degrades performance along the $y$- and $z$-axes. This result further supports the interpretation that the observed failures are structural. When the latent-space objective is misaligned with physical task geometry, executing additional locally optimal planning steps does not lead to accumulated physical progress. Instead, the planner repeatedly selects actions that are optimal in representation space but suboptimal in Cartesian space.

A natural alternative explanation is that the observed axis-dependent behavior is caused by limitations of the camera viewpoint used during planning. To test this, planning was evaluated under alternative camera angles, including a right-side view and an elevated perspective (Appendix~\ref{app:zeros-shot-cam_angle}). Neither alternative improves planning performance. In fact, both substantially degrade performance, even for the $y$-axis where the original viewpoint performs well. This rules out camera placement as the primary cause of the anisotropic behavior and further points to structural properties of the learned representation and dynamics model.

Taken together, these results indicate that zero-shot planning failures arise from two interacting factors: misalignment between the latent representation and physically meaningful task directions, and biases introduced by the learned action-conditioned dynamics. Importantly, these issues cannot be resolved by increasing planning depth or reducing planner randomness alone. Instead, they reflect fundamental limitations of the representation learned under domain shift. This motivates the need for mechanisms that explicitly improve the alignment between latent representations and physical control objectives, which is explored in the following sections.

