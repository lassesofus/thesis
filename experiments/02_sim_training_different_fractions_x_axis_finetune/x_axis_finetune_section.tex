\section{Predictor fine-tuning on simulated x-axis trajectories}

\subsection{Methods}
To investigate whether fine-tuning the action-conditioned predictor on simulation data can improve planning performance along the $x$-axis, we collected a dataset of single-axis reaching trajectories directly in the RoboHive environment and used it to adapt the predictor while keeping the encoder frozen.

\subsubsection{Data generation}
The robot arm is initialized at a fixed starting configuration, and 1000 trajectories are generated by sampling target offsets along the $x$-axis:
\begin{equation*}
    \Delta p^x \sim U(0.05\,\text{m}, 0.3\,\text{m})
\end{equation*}
The target Cartesian position for each trajectory is then $p_g^x = p_0^x + \Delta p^x$, where $p_0$ denotes the starting end-effector position. Each trajectory is executed over a 4.5-second horizon using inverse kinematics with min-jerk smoothing, and video frames are recorded at 30\,fps alongside robot state information (end-effector pose and gripper state). During training, videos are downsampled to 4\,fps.

\subsubsection{Data splits}
The 1000 trajectories are split into 800 training and 200 test samples. To assess data efficiency, the training set is further divided: 640 samples are available for training and 160 for validation. The predictor is trained on 25\%, 50\%, 75\%, and 100\% of the 640 training samples (160, 320, 480, and 640 trajectories respectively).

\subsubsection{Training procedure}
Fine-tuning updates only the action-conditioned predictor while keeping the ViT-g/16 encoder frozen (\texttt{enc\_lr\_scale}$=0$). The pretrained predictor weights from the official V-JEPA~2-AC checkpoint are loaded as initialization. Training uses a learning rate of $10^{-4}$ (approximately 4$\times$ lower than pretraining), AdamW optimizer with weight decay of 0.04, and a cosine annealing schedule with 5 epochs of linear warmup. Early stopping halts training when the validation loss has not improved by at least 0.001 for 10 consecutive epochs.

Importantly, to prevent overfitting to augmentations not applied at inference time, all data augmentation is disabled during training: no horizontal flips, no random cropping scales, and no motion shifts.

\subsubsection{Evaluation protocol}
Evaluation follows the same three-phase protocol as the zero-shot experiments: (1) move to the target position and capture a goal image, (2) return to the starting configuration, and (3) execute 5 steps of CEM-based action planning. The primary metric is the Euclidean distance between the final end-effector position and the target after 5 planning steps. An episode is considered successful if this distance is less than 5\,cm. Each model is evaluated on 10 test samples.

The fine-tuned models are compared against the Meta baseline---the official V-JEPA~2-AC checkpoint applied zero-shot without any simulation-specific adaptation.


\subsection{Results}
Table~\ref{tab:finetune_results} summarizes the evaluation results. Fine-tuning the predictor on simulation data does not improve planning performance along the $x$-axis compared to the zero-shot Meta baseline.

\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Model & Training samples & Mean distance (cm) & Std (cm) & Success rate \\
        \midrule
        Fine-tuned (25\%) & 160 & 19.5 & 5.8 & 0\% \\
        Fine-tuned (50\%) & 320 & 17.1 & 3.9 & 0\% \\
        Fine-tuned (75\%) & 480 & 18.6 & 6.5 & 0\% \\
        Fine-tuned (100\%) & 640 & 19.2 & 6.8 & 0\% \\
        \midrule
        Meta baseline & --- & 18.6 & 5.1 & 0\% \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results for predictor fine-tuning on x-axis trajectories. Mean and standard deviation of final distance to goal after 5 planning steps ($N=10$ episodes). The Meta baseline refers to the official V-JEPA~2-AC checkpoint applied zero-shot. None of the fine-tuned models improve over the baseline, and no episodes achieve the 5\,cm success threshold.}
    \label{tab:finetune_results}
\end{table}

All models---both fine-tuned and baseline---achieve mean final distances in the range of 17--20\,cm, consistent with the $\sim$19\,cm error observed in the original zero-shot $x$-axis experiments. The 50\% fine-tuned model achieves the lowest mean distance (17.1\,cm), but this marginal difference is not statistically significant given the standard deviations. Notably, there is no monotonic improvement with increasing training data: the 100\% model (19.2\,cm) performs slightly worse than the 50\% model.

Figure~\ref{fig:finetune_distance_dist} shows the distribution of final distances across models.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Pictures/Experiments/sim_training_different_fractions_x_axis_finetune/distance_distributions.png}
    \caption{Distribution of final distances to goal after 5 planning steps for fine-tuned models (25\%, 50\%, 75\%, 100\% of training data) and the Meta baseline. All models exhibit similar performance, with median distances around 17--18\,cm and no episodes reaching the 5\,cm success threshold.}
    \label{fig:finetune_distance_dist}
\end{figure}


\subsection{Discussion}
The failure of predictor fine-tuning to improve $x$-axis planning performance is unexpected and warrants careful analysis. Several hypotheses may explain this result.

\subsubsection{Image preprocessing discrepancy}
A potential issue identified in the experimental setup is a mismatch in image preprocessing between training and evaluation. During training, images are scaled by a factor of 1.777 before being cropped to 256$\times$256 pixels (specified via \texttt{random\_resize\_scale: [1.777, 1.777]} in the training configuration). In contrast, evaluation applies no scaling before cropping (\texttt{random\_resize\_scale: (1.0, 1.0)}). This discrepancy means the model sees effectively different ``zoom levels'' during training versus inference: training crops capture a smaller physical region of the scene (higher zoom), while evaluation crops capture a larger region (lower zoom).

If this mismatch is the primary cause of failure, the fine-tuned predictor may have learned representations optimized for the training zoom level that do not transfer to the evaluation setting. This hypothesis requires verification by repeating the evaluation with matching preprocessing parameters.

\subsubsection{Frozen encoder limitations}
Fine-tuning only the predictor while keeping the encoder frozen may be insufficient to overcome the representation--geometry misalignment identified in the zero-shot experiments. The encoder's representations, learned from DROID data, may inherently lack sensitivity to $x$-axis displacements in the simulated camera viewpoint. If the issue lies in the encoder rather than the predictor, adapting only the predictor cannot resolve it.

\subsubsection{Insufficient training data}
With only 640 training trajectories at most, the dataset may be too small to meaningfully adapt the 24-layer predictor. However, the lack of improvement even at 100\% of training data, combined with the use of early stopping to prevent overfitting, suggests that data quantity alone is unlikely to be the limiting factor.

\subsubsection{Inherent x-axis difficulty}
The fundamental camera viewpoint sensitivity documented in the zero-shot experiments may persist regardless of fine-tuning. Motion along the $x$-axis induces lateral image changes that are poorly correlated with Cartesian displacement in the learned representations. If this misalignment is a property of the encoder architecture or pretraining data distribution, predictor-only fine-tuning on a small simulation dataset is unlikely to overcome it.

\subsubsection{Recommendations}
To disambiguate these hypotheses, we recommend:
\begin{enumerate}
    \item Repeating the evaluation with \texttt{random\_resize\_scale: [1.777, 1.777]} to match training preprocessing
    \item Fine-tuning the encoder alongside the predictor (with a smaller learning rate) to test whether encoder adaptation is necessary
    \item Increasing the training dataset size by an order of magnitude to rule out data scarcity
    \item Evaluating on $y$- and $z$-axis trajectories to determine whether the issue is specific to $x$-axis motion or affects all directions
\end{enumerate}
